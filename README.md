# Gentleman Transcripter

## Run Locally

```bash
pnpm install
```

To run the example locally you need to:

1. [Download Ollama](https://ollama.ai/download) and install it locally.
2. run `ollama run llama3` to download and install the model locally (Requires 4.7GB and 8GB of RAM)
3. Open http://localhost:11434 to check if _Ollama is running_.
4. `pnpm install` to install the required dependencies.
5. `pnpm dev` to launch the development server.
